{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fa7eefec-8c7a-40e0-a7e9-07073369617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mmap\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d925c20a-8960-4461-8152-2a3d455c2ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_built() else 'cpu'\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6eef38cc-aaf2-485e-b4e6-f29dbbe72842",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 128\n",
    "max_iters = 3000\n",
    "eval_iters = 100\n",
    "learning_rate = 3e-4\n",
    "n_emb = 384\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "35e4cd71-8069-4f97-b0d2-03c0adddd558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', '#', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] \n",
      "Size: 22\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\", 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab: {chars} \\nSize: {vocab_size}\")\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for ch, i in string_to_int.items()}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: \"\".join([int_to_string[i] for i in l])\n",
    "\n",
    "assert decode(encode(df.heavy.iloc[0])) == df.heavy.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f7b5c102-295e-487e-b0c2-4e031b2678a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"data/train.txt\" if split == 'train' else \"data/val.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a1a54914-5923-464d-86ea-4c04c9c9a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2273e857-2971-4468-b024-3ec3958fca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # B, T, hs\n",
    "        k = self.key(x)   # B, T, hs\n",
    "        w = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5) # B, T, T\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # B, T, T\n",
    "        w = F.softmax(w, dim=-1) # B, T, T\n",
    "        w = self.dropout(w)\n",
    "        v = self.value(x) # B, T, hs\n",
    "        out = w @ v # B, T, T @ B, T, hs -> B, T, hs\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_head*head_size, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # B, T, C\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.mha = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ffw = FeedForward(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.mha(x) # B, T, C\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffw(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_emb) # B, T, C -> B, 1, 1(mean or var) -> B, T, C\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # index, targets are both (B, T) tensor of integers\n",
    "        B, T = index.shape\n",
    "        token_emb = self.token_emb_table(index) # B, T, C\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T).to(device)) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x) # B, T, C\n",
    "        x = self.ln_f(x) # B, T, C\n",
    "        logits = self.lm_head(x) # B, T, Vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, _ = model(index_cond)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1) # B, C\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "            index = torch.cat([index, index_next], dim=1) # B, T+1\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1e0b5fd4-2cfa-4473-a6c9-b4cf8863e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLM(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5b4e3fc3-4605-4749-b17b-56a355bfd2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLM(\n",
       "  (token_emb_table): Embedding(22, 384)\n",
       "  (pos_emb_table): Embedding(64, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffw): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=22, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "70635a25-8bb3-43f3-8a78-60f1425a4172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Train Loss 3.1703 Val Loss 3.1712\n",
      "Step: 100 Train Loss 1.8705 Val Loss 1.8192\n",
      "Step: 200 Train Loss 1.2164 Val Loss 1.1577\n",
      "Step: 300 Train Loss 1.0037 Val Loss 0.9487\n",
      "Step: 400 Train Loss 0.8905 Val Loss 0.8332\n",
      "Step: 500 Train Loss 0.7840 Val Loss 0.7499\n",
      "Step: 600 Train Loss 0.7481 Val Loss 0.6745\n",
      "Step: 700 Train Loss 0.7054 Val Loss 0.6359\n",
      "Step: 800 Train Loss 0.6692 Val Loss 0.5962\n",
      "Step: 900 Train Loss 0.6310 Val Loss 0.5785\n",
      "Step: 1000 Train Loss 0.6054 Val Loss 0.5625\n",
      "Step: 1100 Train Loss 0.5767 Val Loss 0.5272\n",
      "Step: 1200 Train Loss 0.5855 Val Loss 0.5179\n",
      "Step: 1300 Train Loss 0.5718 Val Loss 0.4970\n",
      "Step: 1400 Train Loss 0.5702 Val Loss 0.4930\n",
      "Step: 1500 Train Loss 0.5539 Val Loss 0.4761\n",
      "Step: 1600 Train Loss 0.5388 Val Loss 0.4796\n",
      "Step: 1700 Train Loss 0.5295 Val Loss 0.4337\n",
      "Step: 1800 Train Loss 0.5136 Val Loss 0.4439\n",
      "Step: 1900 Train Loss 0.4618 Val Loss 0.4418\n",
      "Step: 2000 Train Loss 0.5050 Val Loss 0.4590\n",
      "Step: 2100 Train Loss 0.4845 Val Loss 0.4473\n",
      "Step: 2200 Train Loss 0.5032 Val Loss 0.4399\n",
      "Step: 2300 Train Loss 0.4934 Val Loss 0.4203\n",
      "Step: 2400 Train Loss 0.5076 Val Loss 0.4199\n",
      "Step: 2500 Train Loss 0.5113 Val Loss 0.4206\n",
      "Step: 2600 Train Loss 0.4827 Val Loss 0.3985\n",
      "Step: 2700 Train Loss 0.4724 Val Loss 0.4121\n",
      "Step: 2800 Train Loss 0.4475 Val Loss 0.4033\n",
      "Step: 2900 Train Loss 0.4751 Val Loss 0.3901\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f\"Step: {iter} Train Loss {out['train']:.4f} Val Loss {out['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b126f4fb-b6df-4aae-99c2-4de09259b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{iter}.pt\")\n",
    "torch.save({\n",
    "    'iter': iter,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': out\n",
    "}, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "17922564-b973-469d-a8e6-d9f8031ab70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoints on step 2999 sucussessfully, loss is {'train': tensor(0.4751), 'val': tensor(0.3901)}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f\"{checkpoint_dir}/checkpoint_2999.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "iter = checkpoint['iter']\n",
    "loss = checkpoint['loss']\n",
    "print(f\"Loading checkpoints on step {iter} sucussessfully, loss is {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "de821cfd-56dd-4501-9ad6-d6f60c1f3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOT(GPTLM):\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, _ = model(index_cond)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1) # B, C\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "            index = torch.cat([index, index_next], dim=1) # B, T+1\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cbf9932d-2dbc-4c4d-b9be-57bbb42ad8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = BOT(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "09c28749-996d-415d-84f0-72f80a2808bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QVQLKESGPGLVQPSETLSLTCTVSGFSLTSYSVSWVRQPSGKGPEWMGRMWYDDGDTAYNSALKSRLSISRDTSKNQVFLKMNSLQTEDTGTYYCARHTHR'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'QVQLKESGPGLVQPSETLSLTCTVSGFSLTSYSVSWVRQPSGKGPEWMGRMW'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_seq = decode(bot.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
    "generated_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17c0ed-3958-45be-a278-f0c1eabc0789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
